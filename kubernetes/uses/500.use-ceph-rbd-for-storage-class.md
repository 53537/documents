## Use Ceph RBD for Storage Class

[什么是 Storage Class?](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/)



Ceph

- [Intro to Ceph](https://docs.ceph.com/docs/master/start/intro/)
- [Ceph RBD (*Ceph's RADOS Block Devices*)](https://docs.ceph.com/docs/master/rbd/)
- [Ceph RDB (*for storage class*)](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#ceph-rbd)

​		

## 前言

随着越来越多的应用迁移到 Kubernetes 中运行之后，一些有状态应用就需要稳定可靠的分布式网络存储提供数据持久化，这里主要介绍如何通过基于 Ceph RBD 的 Storage Class 进行数据持久化。


## 环境

#### 前置条件

- 首先有一个部署好的 Ceph 集群；
- 其次有一个部署好的 Kubernetes 集群。



## 接入步骤

为 Kubernetes 配置基于 Ceph RBD 的 Stroage Class 需要分为两个步骤进行。


### Step 1. Ceph

*以下步骤在 Ceph 集群节点上执行*



#### Key client.admin

获取 client.admin key

```shell
ceph auth get-key client.admin | base64
```



#### Key client.kube

创建一个用户 kube, 权限： mon = r, osd pool kube = rwx

```shell
ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'
```



获取 client.kebe key

```shell
ceph auth get-key client.kube | base64
```



### Step 2. Kubernetes

*以下步骤在具有 kubectl 的节点上操作*



主要有三个步骤：

- 创建 Storage Class

- 创建 Ceph Admin Secret
- 创建 Ceph Secret



#### 配置并创建 Storage Class

修改编排文件 storage-class.yaml

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: data
provisioner: kubernetes.io/rbd
parameters:
  monitors: 192.168.1.100:6789,192.168.1.101:6789,192.168.1.102:6789
  pool: data
  adminId: admin
  adminSecretNamespace: kube-system
  adminSecretName: ceph-admin-secret
  userId: kube
  userSecretNamespace: kube-system
  userSecretName: ceph-secret
  imageFormat: "2"
  imageFeatures: layering
```



*以下字段配置，请根据自己场景修改*

-  `monitors` 对应的Ceph 集群 Monitor 服务器 IP
- `pool` 对应 ceph rbd pool



更多信息请参阅[Ceph RDB (*for storage class*)](https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#ceph-rbd)



创建 Storage Class

```shell
kubectl create -f storage-class.yaml
```



查看 Storage Class

```shell
kubectl get sc
```





#### 创建 ceph-admin-secret

secret-admin.yaml

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-admin-secret
  namespace: kube-system
type: "kubernetes.io/rbd"
data:
  # ceph auth get-key client.admin | base64
  key: QVFCSzNlUmR1aEFmQmhBQS9LUWlxOERwR2FTbWNLeUxyczNZeHc9PQ==
```

`key` 填入 *client.admin* 的 key, 获取 Key 方式见上方操作。

*ceph-admin-secret 仅创建在 Namespace kube-system 中*



创建 Ceph Admin Secret

```shell
kubectl create -f ceph-admin-secret.yaml
```



#### 创建 ceph-secret

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
  namespace: kube-system
type: "kubernetes.io/rbd"
data:
  # ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'
  # ceph auth get-key client.kube | base64
  key: QVFCWEdveGUwS0dWQWhBQW4wMWZ6U3BpYVp2dkxPR3B3bncrcEE9PQ==
```

`key` 填入 *client.kube* 的 key, 获取 Key 方式见上方操作。

*ceph-secret 创建在所有需要使用此 Storage Class 的 Namespace 中*



创建 Ceph Secret

```shell
kubectl create -f ceph-secret.yaml
```



### Demo

本段通过接入两个不同类型的服务，来演示 *Deployment* 和 *StatefulSet* 下如何使用 Storage Class 进行数据持久化。



#### Deployment

部署一个 Redis 服务

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: data-redis-myredis
  annotations:
    volume.beta.kubernetes.io/storage-class: data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Service

metadata:
  name: myredis

spec:
  ports:
  - port: 6379
  selector:
    app: myredis
---
apiVersion: apps/v1
kind: Deployment

metadata:
  name: myredis

spec:
  replicas: 1
  selector:
    matchLabels:
      app: myredis
  template:
    metadata:
      labels:
        app: myredis
    spec:
      containers:
      - name: redis
        image: statemood/redis:5.0.7
        imagePullPolicy: Always
        ports:
        - containerPort: 6379
        resources:
          requests:
            cpu: 300m
            memory: 1Gi
          limits:
            cpu: 300m
            memory: 1Gi
        livenessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 10
          timeoutSeconds: 3
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 10
          timeoutSeconds: 3
          periodSeconds: 10
        volumeMounts:
        - name: data
          mountPath: /var/lib/redis
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: data-redis-myredis
```



#### StatefulSet

部署 Zookeeper 集群

```yaml
apiVersion: v1
kind: Service
metadata:
  name: zk-svc
  labels:
    app: zk-svc
spec:
  ports:
  - port: 2888
    name: server
  - port: 3888
    name: leader-election
  clusterIP: None
  selector:
    app: zk
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: zk-cm
data:
  jvm.heap: "1000"
  tick: "2000"
  init: "10"
  sync: "5"
  client.cnxns: "300"
  snap.retain: "7"
  purge.interval: "24"
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: zk-pdb
spec:
  selector:
    matchLabels:
      app: zk
  minAvailable: 2
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zk
spec:
  serviceName: zk-svc
  replicas: 3
  selector:
    matchLabels:
      app: zk
  template:
    metadata:
      labels:
        app: zk
    spec:
      initContainers:
      - name: init-dir
        image: statemood/alpine:3.11
        command:
        - /bin/sh
        - -c
        - mkdir -p /data/zk && chown -v 567. /data/zk
        volumeMounts:
        - name: data
          mountPath: /data
      containers:
      - name: zk
        imagePullPolicy: Always
        image: statemood/zookeeper:3.6.0
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        - containerPort: 8080
          name: admin
        env:
        - name : ZK_REPLICAS
          value: "3"
        - name:  ZK_4LW_COMMANDS_WHITELIST
          value: "ruok, cons, stat, mntr"
        - name : ZK_SERVER_HEAP
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: jvm.heap
        - name : ZK_TICK_TIME
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: tick
        - name : ZK_INIT_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: init
        - name : ZK_SYNC_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: sync
        - name : ZK_MAX_CLIENT_CNXNS
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: client.cnxns
        - name: ZK_SNAP_RETAIN_COUNT
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: snap.retain
        - name: ZK_PURGE_INTERVAL
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: purge.interval
        - name: ZK_LOG_DIR
          value: "/tmp"
        - name: ZK_CLIENT_PORT
          value: "2181"
        - name: ZK_SERVER_PORT
          value: "2888"
        - name: ZK_ELECTION_PORT
          value: "3888"
        readinessProbe:
          exec:
            command:
            - "zkOk.sh"
          initialDelaySeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
            - "zkOk.sh"
          initialDelaySeconds: 10
          timeoutSeconds: 5
        volumeMounts:
        - name: data
          mountPath: /data
        securityContext:
          runAsUser: 567
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      storageClassName: data
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
```

